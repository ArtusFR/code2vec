{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7df31f",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99182c6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'summary' from 'torch_geometric.utils' (C:\\Users\\ARTUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'summary' from 'torch_geometric.utils' (C:\\Users\\ARTUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import random \n",
    "import pickle\n",
    "\n",
    "import models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bebb78",
   "metadata": {},
   "source": [
    "# Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153093ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "\n",
    "SEED = 1234\n",
    "DATA_DIR = 'data'\n",
    "DATASET = 'java-small'\n",
    "EMBEDDING_DIM = 128\n",
    "DROPOUT = 0.25\n",
    "BATCH_SIZE = 256\n",
    "CHUNKS = 10\n",
    "MAX_LENGTH = 200\n",
    "LOG_EVERY = 100 #print log of results after every LOG_EVERY batches\n",
    "N_EPOCHS = 50\n",
    "LOG_DIR = 'logs'\n",
    "SAVE_DIR = 'checkpoints'\n",
    "LOG_PATH = os.path.join(LOG_DIR, f'{DATASET}-log.txt')\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, f'{DATASET}-model.pt')\n",
    "LOAD = False #set true if you want to load model from MODEL_SAVE_PATH\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c62c7e",
   "metadata": {},
   "source": [
    "# Seed fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f4a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb550c4",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c5517",
   "metadata": {},
   "source": [
    "## Dict des word (variables), path, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65ddeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_DIR}/{DATASET}/{DATASET}.dict.c2v', 'rb') as file:\n",
    "    word2count = pickle.load(file)\n",
    "    path2count = pickle.load(file)\n",
    "    target2count = pickle.load(file)\n",
    "    n_training_examples = pickle.load(file)\n",
    "\n",
    "# create vocabularies, initialized with unk and pad tokens\n",
    "\n",
    "word2idx = {'<unk>': 0, '<pad>': 1}\n",
    "path2idx = {'<unk>': 0, '<pad>': 1 }\n",
    "target2idx = {'<unk>': 0, '<pad>': 1}\n",
    "\n",
    "for w in word2count.keys():\n",
    "    word2idx[w] = len(word2idx)\n",
    "\n",
    "for p in path2count.keys():\n",
    "    path2idx[p] = len(path2idx)\n",
    "\n",
    "for t in target2count.keys():\n",
    "    target2idx[t] = len(target2idx)\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "idx2path = {v: k for k, v in path2idx.items()}\n",
    "idx2target = {v: k for k, v in target2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75068c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11318"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459ebe3",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c208b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        a = [line.strip() for line in f]\n",
    "        b = map(lambda l: (l.split(' ')[0], \n",
    "                                    [t.split(',') for t in l.split(' ')[1:] if t.strip()]), a)\n",
    "        return [i for i in b if len(i[1]) <= MAX_LENGTH ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3000e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = load_data(f'{DATA_DIR}/{DATASET}/{DATASET}.test.c2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0923d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = load_data(f'{DATA_DIR}/{DATASET}/{DATASET}.val.c2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b125651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_data(f'{DATA_DIR}/{DATASET}/{DATASET}.train.c2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "877d8370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57088, 23844, 555075)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test), len(data_val), len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3940d597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ac22d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    name_idx = torch.tensor([target2idx.get(e[0], target2idx['<unk>']) for e in samples])\n",
    "\n",
    "    path_context_idx = [\n",
    "        ([word2idx.get(l, word2idx['<unk>']) for l, p, r in e[1]], \n",
    "         [path2idx.get(p, path2idx['<unk>']) for l, p, r in e[1]],\n",
    "         [word2idx.get(r, word2idx['<unk>']) for l, p, r in e[1]])\n",
    "        for e in samples\n",
    "    ]\n",
    "    \n",
    "    # Determine max length for padding\n",
    "    max_length = max(map(lambda e: len(e[0]), path_context_idx))\n",
    "\n",
    "    # Pad sequences\n",
    "    def pad_sequence(sequences, pad_value):\n",
    "        return [seq + [pad_value] * (max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "    left_tensor = torch.tensor(pad_sequence([e[0] for e in path_context_idx], word2idx['<pad>']))\n",
    "    path_tensor = torch.tensor(pad_sequence([e[1] for e in path_context_idx], path2idx['<pad>']))\n",
    "    right_tensor = torch.tensor(pad_sequence([e[2] for e in path_context_idx], word2idx['<pad>']))\n",
    "\n",
    "    return name_idx, left_tensor, path_tensor, right_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c5743f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size=BATCH_SIZE, collate_fn=collate_fn,\n",
    "                          pin_memory=True, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(data_test, batch_size=BATCH_SIZE, collate_fn=collate_fn, \n",
    "                         pin_memory=True, shuffle=False, num_workers=0)\n",
    "eval_loader = DataLoader(data_val, batch_size=BATCH_SIZE, collate_fn=collate_fn, \n",
    "                         pin_memory=True, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71ce7aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465970"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for i in train_loader:\n",
    "    c += i[0].eq(0).sum().item()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037fb7d",
   "metadata": {},
   "source": [
    "# Instanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5957a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Code2Vec(len(word2idx), len(path2idx), EMBEDDING_DIM, len(target2idx), DROPOUT).to(device)\n",
    "\n",
    "if LOAD:\n",
    "    print(f'Loading model from {MODEL_SAVE_PATH}')\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "286aafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "if not os.path.isdir(f'{LOG_DIR}'):\n",
    "    os.makedirs(f'{LOG_DIR}')\n",
    "\n",
    "if os.path.exists(LOG_PATH):\n",
    "    os.remove(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5d86f",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "226a7bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Code2Vec(\n",
      "  (node_embedding): Embedding(73906, 128)\n",
      "  (path_embedding): Embedding(323, 128)\n",
      "  (out): Linear(in_features=128, out_features=11318, bias=True)\n",
      "  (do): Dropout(p=0.25, inplace=False)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d152f5f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37327b2",
   "metadata": {},
   "source": [
    "## métrique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a814d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(fx, y):\n",
    "    \"\"\"\n",
    "    Calculate top-1 accuracy\n",
    "\n",
    "    fx = [batch size, output dim]\n",
    "     y = [batch size]\n",
    "    \"\"\"\n",
    "    pred_idxs = fx.max(1, keepdim=True)[1]\n",
    "    correct = pred_idxs.eq(y.view_as(pred_idxs)).sum()\n",
    "    acc = correct.float()/pred_idxs.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def calculate_f1(fx, y):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall and F1 score\n",
    "    - Takes top-1 predictions\n",
    "    - Converts to strings\n",
    "    - Splits into sub-tokens\n",
    "    - Calculates TP, FP and FN\n",
    "    - Calculates precision, recall and F1 score\n",
    "\n",
    "    fx = [batch size, output dim]\n",
    "     y = [batch size]\n",
    "    \"\"\"\n",
    "    pred_idxs = fx.max(1, keepdim=True)[1]\n",
    "    pred_names = [idx2target[i.item()] for i in pred_idxs]\n",
    "    original_names = [idx2target[i.item()] for i in y]\n",
    "    true_positive, false_positive, false_negative = 0, 0, 0\n",
    "    for p, o in zip(pred_names, original_names):\n",
    "        predicted_subtokens = p.split('|')\n",
    "        original_subtokens = o.split('|')\n",
    "        for subtok in predicted_subtokens:\n",
    "            if subtok in original_subtokens:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "        for subtok in original_subtokens:\n",
    "            if not subtok in predicted_subtokens:\n",
    "                false_negative += 1\n",
    "    try:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        precision, recall, f1 = 0, 0, 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def get_metrics(tensor_n, tensor_l, tensor_p, tensor_r, model, criterion):\n",
    "    \"\"\"\n",
    "    Takes inputs, calculates loss, accuracy and other metrics, then calculates gradients and updates parameters\n",
    "\n",
    "    if optimizer is None, then we are doing evaluation so no gradients are calculated and no parameters are updated\n",
    "    \"\"\"\n",
    "\n",
    "    fx = model(tensor_l, tensor_p, tensor_r)\n",
    "\n",
    "    loss = criterion(fx, tensor_n)\n",
    "\n",
    "    acc = calculate_accuracy(fx, tensor_n)\n",
    "    precision, recall, f1 = calculate_f1(fx, tensor_n)\n",
    "\n",
    "    return loss, acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0cdfd",
   "metadata": {},
   "source": [
    "## Eval func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31af8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluation loop using DataLoader.\n",
    "    Wraps computations in `torch.no_grad()` to avoid unnecessary gradient calculations.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    epoch_loss, epoch_acc, epoch_p, epoch_r, epoch_f1 = 0, 0, 0, 0, 0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tensor_n, tensor_l, tensor_p, tensor_r in eval_loader:\n",
    "            # Move tensors to GPU\n",
    "            tensor_n = tensor_n.to(device)\n",
    "            tensor_l = tensor_l.to(device)\n",
    "            tensor_p = tensor_p.to(device)\n",
    "            tensor_r = tensor_r.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            loss, acc, p, r, f1 = get_metrics(tensor_n, tensor_l, tensor_p, tensor_r, model, criterion)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "            epoch_p += p\n",
    "            epoch_r += r\n",
    "            epoch_f1 += f1\n",
    "            n_batches += 1\n",
    "\n",
    "            if n_batches % LOG_EVERY == 0:\n",
    "                log = f\"\\t| Batches: {n_batches} |\\n\"\n",
    "                log += f\"\\t| Loss: {epoch_loss / n_batches:.3f} | Acc.: {epoch_acc / n_batches:.3f} | P: {epoch_p / n_batches:.3f} | R: {epoch_r / n_batches:.3f} | F1: {epoch_f1 / n_batches:.3f}\"\n",
    "                \n",
    "                with open(LOG_PATH, 'a+') as f:\n",
    "                    f.write(log + '\\n')\n",
    "                print(log)\n",
    "\n",
    "    return epoch_loss / n_batches, epoch_acc / n_batches, epoch_p / n_batches, epoch_r / n_batches, epoch_f1 / n_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd90ca2",
   "metadata": {},
   "source": [
    "## Training func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92ef6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop using DataLoader for batch streaming\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss, epoch_acc, epoch_p, epoch_r, epoch_f1 = 0, 0, 0, 0, 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for tensor_n, tensor_l, tensor_p, tensor_r in train_loader:\n",
    "        # Move tensors to GPU\n",
    "        tensor_n = tensor_n.to(device)\n",
    "        tensor_l = tensor_l.to(device)\n",
    "        tensor_p = tensor_p.to(device)\n",
    "        tensor_r = tensor_r.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        # loss, acc, p, r, f1 = get_metrics(tensor_n, tensor_l, tensor_p, tensor_r, model, criterion)\n",
    "        \n",
    "        fx = model(tensor_l, tensor_p, tensor_r)\n",
    "        loss = criterion(fx, tensor_n)\n",
    "        acc = calculate_accuracy(fx, tensor_n)\n",
    "        p, r, f1 = calculate_f1(fx, tensor_n)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics\n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        epoch_p += p\n",
    "        epoch_r += r\n",
    "        epoch_f1 += f1\n",
    "        n_batches += 1\n",
    "\n",
    "        if n_batches % LOG_EVERY == 0:\n",
    "            log = f\"\\t| Batches: {n_batches} | Completion: {((n_batches*BATCH_SIZE)/len(data_train))*100:.3f}% |\\n\"\n",
    "            log += f\"\\t| Loss: {epoch_loss / n_batches:.3f} | Acc.: {epoch_acc / n_batches:.3f} | P: {epoch_p / n_batches:.3f} | R: {epoch_r / n_batches:.3f} | F1: {epoch_f1 / n_batches:.3f}\"\n",
    "            with open(LOG_PATH, 'a+') as f:\n",
    "                f.write(log + '\\n')\n",
    "            print(log)\n",
    "\n",
    "    return epoch_loss / n_batches, epoch_acc / n_batches, epoch_p / n_batches, epoch_r / n_batches, epoch_f1 / n_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c867a1",
   "metadata": {},
   "source": [
    "## Log func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89dc92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logfunc(log):\n",
    "    with open(LOG_PATH, 'a+') as f:\n",
    "        f.write(log+'\\n')\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d5847",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc8a25ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 - Training\n",
      "\t| Batches: 100 | Completion: 4.612% |\n",
      "\t| Loss: 2.946 | Acc.: 0.732 | P: 0.732 | R: 0.732 | F1: 0.732\n",
      "\t| Batches: 200 | Completion: 9.224% |\n",
      "\t| Loss: 2.140 | Acc.: 0.784 | P: 0.784 | R: 0.784 | F1: 0.784\n",
      "\t| Batches: 300 | Completion: 13.836% |\n",
      "\t| Loss: 1.866 | Acc.: 0.802 | P: 0.802 | R: 0.802 | F1: 0.802\n",
      "\t| Batches: 400 | Completion: 18.448% |\n",
      "\t| Loss: 1.732 | Acc.: 0.810 | P: 0.810 | R: 0.810 | F1: 0.810\n",
      "\t| Batches: 500 | Completion: 23.060% |\n",
      "\t| Loss: 1.643 | Acc.: 0.816 | P: 0.816 | R: 0.816 | F1: 0.816\n",
      "\t| Batches: 600 | Completion: 27.672% |\n",
      "\t| Loss: 1.586 | Acc.: 0.820 | P: 0.820 | R: 0.820 | F1: 0.820\n",
      "\t| Batches: 700 | Completion: 32.284% |\n",
      "\t| Loss: 1.542 | Acc.: 0.823 | P: 0.823 | R: 0.823 | F1: 0.823\n",
      "\t| Batches: 800 | Completion: 36.896% |\n",
      "\t| Loss: 1.508 | Acc.: 0.825 | P: 0.825 | R: 0.825 | F1: 0.825\n",
      "\t| Batches: 900 | Completion: 41.508% |\n",
      "\t| Loss: 1.488 | Acc.: 0.826 | P: 0.826 | R: 0.826 | F1: 0.826\n",
      "\t| Batches: 1000 | Completion: 46.120% |\n",
      "\t| Loss: 1.466 | Acc.: 0.828 | P: 0.828 | R: 0.828 | F1: 0.828\n",
      "\t| Batches: 1100 | Completion: 50.732% |\n",
      "\t| Loss: 1.448 | Acc.: 0.829 | P: 0.829 | R: 0.829 | F1: 0.829\n",
      "\t| Batches: 1200 | Completion: 55.344% |\n",
      "\t| Loss: 1.436 | Acc.: 0.830 | P: 0.830 | R: 0.830 | F1: 0.830\n",
      "\t| Batches: 1300 | Completion: 59.956% |\n",
      "\t| Loss: 1.423 | Acc.: 0.830 | P: 0.830 | R: 0.830 | F1: 0.830\n",
      "\t| Batches: 1400 | Completion: 64.568% |\n",
      "\t| Loss: 1.412 | Acc.: 0.831 | P: 0.831 | R: 0.831 | F1: 0.831\n",
      "\t| Batches: 1500 | Completion: 69.180% |\n",
      "\t| Loss: 1.402 | Acc.: 0.832 | P: 0.832 | R: 0.832 | F1: 0.832\n",
      "\t| Batches: 1600 | Completion: 73.792% |\n",
      "\t| Loss: 1.395 | Acc.: 0.832 | P: 0.832 | R: 0.832 | F1: 0.832\n",
      "\t| Batches: 1700 | Completion: 78.404% |\n",
      "\t| Loss: 1.389 | Acc.: 0.832 | P: 0.832 | R: 0.832 | F1: 0.832\n",
      "\t| Batches: 1800 | Completion: 83.016% |\n",
      "\t| Loss: 1.384 | Acc.: 0.832 | P: 0.832 | R: 0.832 | F1: 0.832\n",
      "\t| Batches: 1900 | Completion: 87.628% |\n",
      "\t| Loss: 1.378 | Acc.: 0.833 | P: 0.833 | R: 0.833 | F1: 0.833\n",
      "\t| Batches: 2000 | Completion: 92.240% |\n",
      "\t| Loss: 1.372 | Acc.: 0.833 | P: 0.833 | R: 0.833 | F1: 0.833\n",
      "\t| Batches: 2100 | Completion: 96.852% |\n",
      "\t| Loss: 1.367 | Acc.: 0.834 | P: 0.834 | R: 0.834 | F1: 0.834\n",
      "Epoch: 01 - Validation\n",
      "\t| Batches: 100 |\n",
      "\t| Loss: 1.595 | Acc.: 0.817 | P: 0.817 | R: 0.817 | F1: 0.817\n",
      "\t| Batches: 200 |\n",
      "\t| Loss: 2.661 | Acc.: 0.755 | P: 0.755 | R: 0.755 | F1: 0.755\n",
      "| Epoch: 01 |\n",
      "| Train Loss: 1.365 | Train Precision: 0.834 | Train Recall: 0.834 | Train F1: 0.834 | Train Acc: 83.37% |\n",
      "| Val. Loss: 2.530 | Val. Precision: 0.765 | Val. Recall: 0.765 | Val. F1: 0.765 | Val. Acc: 76.46% |\n",
      "Epoch: 02 - Training\n",
      "\t| Batches: 100 | Completion: 4.612% |\n",
      "\t| Loss: 1.264 | Acc.: 0.839 | P: 0.839 | R: 0.839 | F1: 0.839\n",
      "\t| Batches: 200 | Completion: 9.224% |\n",
      "\t| Loss: 1.263 | Acc.: 0.840 | P: 0.840 | R: 0.840 | F1: 0.840\n",
      "\t| Batches: 300 | Completion: 13.836% |\n",
      "\t| Loss: 1.255 | Acc.: 0.841 | P: 0.841 | R: 0.841 | F1: 0.841\n",
      "\t| Batches: 400 | Completion: 18.448% |\n",
      "\t| Loss: 1.256 | Acc.: 0.840 | P: 0.840 | R: 0.840 | F1: 0.840\n",
      "\t| Batches: 500 | Completion: 23.060% |\n",
      "\t| Loss: 1.244 | Acc.: 0.840 | P: 0.840 | R: 0.840 | F1: 0.840\n",
      "\t| Batches: 600 | Completion: 27.672% |\n",
      "\t| Loss: 1.228 | Acc.: 0.843 | P: 0.843 | R: 0.843 | F1: 0.843\n",
      "\t| Batches: 700 | Completion: 32.284% |\n",
      "\t| Loss: 1.218 | Acc.: 0.845 | P: 0.845 | R: 0.845 | F1: 0.845\n",
      "\t| Batches: 800 | Completion: 36.896% |\n",
      "\t| Loss: 1.211 | Acc.: 0.846 | P: 0.846 | R: 0.846 | F1: 0.846\n",
      "\t| Batches: 900 | Completion: 41.508% |\n",
      "\t| Loss: 1.203 | Acc.: 0.847 | P: 0.847 | R: 0.847 | F1: 0.847\n",
      "\t| Batches: 1000 | Completion: 46.120% |\n",
      "\t| Loss: 1.196 | Acc.: 0.848 | P: 0.848 | R: 0.848 | F1: 0.848\n",
      "\t| Batches: 1100 | Completion: 50.732% |\n",
      "\t| Loss: 1.191 | Acc.: 0.848 | P: 0.848 | R: 0.848 | F1: 0.848\n",
      "\t| Batches: 1200 | Completion: 55.344% |\n",
      "\t| Loss: 1.185 | Acc.: 0.849 | P: 0.849 | R: 0.849 | F1: 0.849\n",
      "\t| Batches: 1300 | Completion: 59.956% |\n",
      "\t| Loss: 1.184 | Acc.: 0.849 | P: 0.849 | R: 0.849 | F1: 0.849\n",
      "\t| Batches: 1400 | Completion: 64.568% |\n",
      "\t| Loss: 1.180 | Acc.: 0.850 | P: 0.850 | R: 0.850 | F1: 0.850\n",
      "\t| Batches: 1500 | Completion: 69.180% |\n",
      "\t| Loss: 1.177 | Acc.: 0.850 | P: 0.850 | R: 0.850 | F1: 0.850\n",
      "\t| Batches: 1600 | Completion: 73.792% |\n",
      "\t| Loss: 1.173 | Acc.: 0.850 | P: 0.850 | R: 0.850 | F1: 0.850\n",
      "\t| Batches: 1700 | Completion: 78.404% |\n",
      "\t| Loss: 1.170 | Acc.: 0.851 | P: 0.851 | R: 0.851 | F1: 0.851\n",
      "\t| Batches: 1800 | Completion: 83.016% |\n",
      "\t| Loss: 1.166 | Acc.: 0.851 | P: 0.851 | R: 0.851 | F1: 0.851\n",
      "\t| Batches: 1900 | Completion: 87.628% |\n",
      "\t| Loss: 1.162 | Acc.: 0.851 | P: 0.851 | R: 0.851 | F1: 0.851\n",
      "\t| Batches: 2000 | Completion: 92.240% |\n",
      "\t| Loss: 1.159 | Acc.: 0.851 | P: 0.851 | R: 0.851 | F1: 0.851\n",
      "\t| Batches: 2100 | Completion: 96.852% |\n",
      "\t| Loss: 1.154 | Acc.: 0.852 | P: 0.852 | R: 0.852 | F1: 0.852\n",
      "Epoch: 02 - Validation\n",
      "\t| Batches: 100 |\n",
      "\t| Loss: 1.479 | Acc.: 0.817 | P: 0.817 | R: 0.817 | F1: 0.817\n",
      "\t| Batches: 200 |\n",
      "\t| Loss: 2.623 | Acc.: 0.755 | P: 0.755 | R: 0.755 | F1: 0.755\n",
      "| Epoch: 02 |\n",
      "| Train Loss: 1.152 | Train Precision: 0.852 | Train Recall: 0.852 | Train F1: 0.852 | Train Acc: 85.18% |\n",
      "| Val. Loss: 2.484 | Val. Precision: 0.764 | Val. Recall: 0.764 | Val. F1: 0.764 | Val. Acc: 76.42% |\n",
      "Epoch: 03 - Training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m      4\u001b[0m     logfunc(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     train_loss, train_acc, train_p, train_r, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     logfunc(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Validation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     valid_loss, valid_acc, valid_p, valid_r, valid_f1 \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[31], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m epoch_loss, epoch_acc, epoch_p, epoch_r, epoch_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m n_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_r\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move tensors to GPU\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_n\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_n\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_l\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 19\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(samples)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [seq \u001b[38;5;241m+\u001b[39m [pad_value] \u001b[38;5;241m*\u001b[39m (max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq)) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences]\n\u001b[0;32m     18\u001b[0m left_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(pad_sequence([e[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m path_context_idx], word2idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m---> 19\u001b[0m path_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_context_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath2idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<pad>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m right_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(pad_sequence([e[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m path_context_idx], word2idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m name_idx, left_tensor, path_tensor, right_tensor\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    logfunc(f\"Epoch: {epoch+1:02} - Training\")\n",
    "    train_loss, train_acc, train_p, train_r, train_f1 = train(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    logfunc(f\"Epoch: {epoch+1:02} - Validation\")\n",
    "    valid_loss, valid_acc, valid_p, valid_r, valid_f1 = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "    log = f\"| Epoch: {epoch+1:02} |\\n\"\n",
    "    log += f\"| Train Loss: {train_loss:.3f} | Train Precision: {train_p:.3f} | Train Recall: {train_r:.3f} | Train F1: {train_f1:.3f} | Train Acc: {train_acc * 100:.2f}% |\\n\"\n",
    "    log += f\"| Val. Loss: {valid_loss:.3f} | Val. Precision: {valid_p:.3f} | Val. Recall: {valid_r:.3f} | Val. F1: {valid_f1:.3f} | Val. Acc: {valid_acc * 100:.2f}% |\"\n",
    "    logfunc(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7452ba",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba75b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "logfunc('Testing')\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "test_loss, test_acc, test_p, test_r, test_f1 = evaluate(model, eval_loader, criterion, device)\n",
    "\n",
    "logfunc(f'| Test Loss: {test_loss:.3f} | Test Precision: {test_p:.3f} | Test Recall: {test_r:.3f} | Test F1: {test_f1:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
