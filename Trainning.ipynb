{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7df31f",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99182c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random \n",
    "import pickle\n",
    "\n",
    "import models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from torch_geometric.nn import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bebb78",
   "metadata": {},
   "source": [
    "# Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "153093ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "\n",
    "SEED = 1234\n",
    "DATA_DIR = 'data'\n",
    "DATASET_PATH = 'java-small-preprocessed-code2vec/java-small'\n",
    "DATASET_NAME = 'java-small'\n",
    "EMBEDDING_DIM = 128\n",
    "DROPOUT = 0.25\n",
    "BATCH_SIZE = 128\n",
    "CHUNKS = 10\n",
    "MAX_LENGTH = 200\n",
    "LOG_EVERY = 1000 #print log of results after every LOG_EVERY batches\n",
    "N_EPOCHS = 20\n",
    "START_EPOCHS = 1\n",
    "LOG_DIR = 'logs'\n",
    "SAVE_DIR = 'checkpoints'\n",
    "LOG_PATH = os.path.join(LOG_DIR, f'{DATASET_NAME}-log.txt')\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, f'{DATASET_NAME}-{START_EPOCHS:02}-model.pt')\n",
    "LOAD = True #set true if you want to load model from MODEL_SAVE_PATH\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acdcb15",
   "metadata": {},
   "source": [
    "## Log func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f1e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logfunc(log):\n",
    "    with open(LOG_PATH, 'a+') as f:\n",
    "        f.write(log+'\\n')\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc034d",
   "metadata": {},
   "source": [
    "## Dir init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea7ded0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if os.path.exists(LOG_PATH):\\n    os.remove(LOG_PATH) '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "if not os.path.isdir(f'{LOG_DIR}'):\n",
    "    os.makedirs(f'{LOG_DIR}')\n",
    "\n",
    "\"\"\" if os.path.exists(LOG_PATH):\n",
    "    os.remove(LOG_PATH) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c62c7e",
   "metadata": {},
   "source": [
    "# Seed fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f4a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb550c4",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c5517",
   "metadata": {},
   "source": [
    "## Dict des word (variables), path, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65ddeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_DIR}/{DATASET_PATH}/{DATASET_NAME}.dict.c2v', 'rb') as file:\n",
    "    word2count = pickle.load(file)\n",
    "    path2count = pickle.load(file)\n",
    "    target2count = pickle.load(file)\n",
    "    n_training_examples = pickle.load(file)\n",
    "\n",
    "# create vocabularies, initialized with unk and pad tokens\n",
    "\n",
    "word2idx = {'<unk>': 0, '<pad>': 1}\n",
    "path2idx = {'<unk>': 0, '<pad>': 1 }\n",
    "target2idx = {'<unk>': 0, '<pad>': 1}\n",
    "\n",
    "for w in word2count.keys():\n",
    "    word2idx[w] = len(word2idx)\n",
    "\n",
    "for p in path2count.keys():\n",
    "    path2idx[p] = len(path2idx)\n",
    "\n",
    "for t in target2count.keys():\n",
    "    target2idx[t] = len(target2idx)\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "idx2path = {v: k for k, v in path2idx.items()}\n",
    "idx2target = {v: k for k, v in target2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f6ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75068c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199749, 507272, 807139)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2target), len(idx2word), len(idx2path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27702a61",
   "metadata": {},
   "source": [
    "## File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c208b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [\n",
    "            (line.split(' ')[0], [t.split(',') for t in line.split(' ')[1:] if t.strip()])\n",
    "            for line in f if len(line.split(' ')) - 1 <= MAX_LENGTH\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b66ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            parts = line.strip().split(' ')\n",
    "            if len(parts) - 1 > MAX_LENGTH:\n",
    "                continue\n",
    "            \n",
    "            name = target2idx.get(parts[0], target2idx['<unk>'])\n",
    "            \n",
    "            path_contexts = [tuple(t.split(',')) for t in parts[1:] if t.strip()]\n",
    "            left, path, right = zip(*path_contexts) if path_contexts else ([], [], [])\n",
    "            \n",
    "            left_tensor = torch.tensor([word2idx.get(l, word2idx['<unk>']) for l in left], dtype=torch.long)\n",
    "            path_tensor = torch.tensor([path2idx.get(p, path2idx['<unk>']) for p in path], dtype=torch.long)\n",
    "            right_tensor = torch.tensor([word2idx.get(r, word2idx['<unk>']) for r in right], dtype=torch.long)\n",
    "\n",
    "            data.append((torch.tensor(name, dtype=torch.long), left_tensor, path_tensor, right_tensor))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3000e573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb34d8a240504048a06d1af3027c4701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_test = load_data(f'{DATA_DIR}/{DATASET_PATH}/{DATASET_NAME}.test.c2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd0923d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153e08752b654481bc0bd95a8c0a2c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_val = load_data(f'{DATA_DIR}/{DATASET_PATH}/{DATASET_NAME}.val.c2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b125651b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720168f573cf4508b30ad5e7dc4dc0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train = load_data(f'{DATA_DIR}/{DATASET_PATH}/{DATASET_NAME}.train.c2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "877d8370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56165, 23505, 665115)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test), len(data_val), len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3940d597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665115"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_training_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f74d5",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f26ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    name_idx = torch.stack([e[0] for e in samples])\n",
    "    \n",
    "    max_length = max(len(e[1]) for e in samples)\n",
    "    \n",
    "    def pad_tensor(tensor_list, pad_value):\n",
    "        return torch.stack([torch.cat([t, torch.full((max_length - len(t),), pad_value)]) for t in tensor_list])\n",
    "\n",
    "    left_tensor = pad_tensor([e[1] for e in samples], word2idx['<pad>'])\n",
    "    path_tensor = pad_tensor([e[2] for e in samples], path2idx['<pad>'])\n",
    "    right_tensor = pad_tensor([e[3] for e in samples], word2idx['<pad>'])\n",
    "\n",
    "    return name_idx, left_tensor, path_tensor, right_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c5743f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size=BATCH_SIZE, collate_fn=collate_fn,\n",
    "                          pin_memory=True, shuffle=True, num_workers=0, prefetch_factor=None)\n",
    "test_loader = DataLoader(data_test, batch_size=BATCH_SIZE, collate_fn=collate_fn, \n",
    "                         pin_memory=True, shuffle=False, num_workers=0, prefetch_factor=None)\n",
    "eval_loader = DataLoader(data_val, batch_size=BATCH_SIZE, collate_fn=collate_fn, \n",
    "                         pin_memory=True, shuffle=False, num_workers=0, prefetch_factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae5bf286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5197, 439, 184)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader), len(eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71ce7aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5085e52e61d948d8a92e5b4230831455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 625202, 0]\n"
     ]
    }
   ],
   "source": [
    "c = [0 for i in range(4)]\n",
    "for ts in tqdm(train_loader):\n",
    "    for j, t in enumerate(ts):\n",
    "        c[j] += t.eq(0).sum().item()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1a75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del c, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6520f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 852233)\n"
     ]
    }
   ],
   "source": [
    "m, Ma = sys.maxsize, 0\n",
    "for v in path2count.values():\n",
    "    m, Ma = min(m,v), max(Ma,v)\n",
    "print((m, Ma))\n",
    "del m, Ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb39cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.desc = \"train\"\n",
    "test_loader.desc = \"test\"\n",
    "eval_loader.desc = \"eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe929a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable              Type              Data/Info\n",
      "-------------------------------------------------\n",
      "BATCH_SIZE            int               128\n",
      "CHUNKS                int               10\n",
      "DATASET_NAME          str               java-small\n",
      "DATASET_PATH          str               java-small-preprocessed-code2vec/java-small\n",
      "DATA_DIR              str               data\n",
      "DROPOUT               float             0.25\n",
      "DataLoader            type              <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "EMBEDDING_DIM         int               128\n",
      "F                     module            <module 'torch.nn.functio<...>orch\\\\nn\\\\functional.py'>\n",
      "LOAD                  bool              True\n",
      "LOG_DIR               str               logs\n",
      "LOG_EVERY             int               1000\n",
      "LOG_PATH              str               logs\\java-small-log.txt\n",
      "MAX_LENGTH            int               200\n",
      "MODEL_SAVE_PATH       str               checkpoints\\java-small-1-model.pt\n",
      "N_EPOCHS              int               20\n",
      "Pool                  method            <bound method BaseContext<...>t at 0x000001E62C830200>>\n",
      "SAVE_DIR              str               checkpoints\n",
      "SEED                  int               1234\n",
      "START_EPOCHS          int               1\n",
      "collate_fn            function          <function collate_fn at 0x000001E641ADA340>\n",
      "data_test             list              n=56165\n",
      "data_train            list              n=665115\n",
      "data_val              list              n=23505\n",
      "device                device            cuda\n",
      "eval_loader           DataLoader        <torch.utils.data.dataloa<...>ct at 0x000001E636272300>\n",
      "file                  BufferedReader    <_io.BufferedReader name=<...>all/java-small.dict.c2v'>\n",
      "idx2path              dict              n=807139\n",
      "idx2target            dict              n=199749\n",
      "idx2word              dict              n=507272\n",
      "j                     int               3\n",
      "load_data             function          <function load_data at 0x000001E63863DF80>\n",
      "logfunc               function          <function logfunc at 0x000001E63860B920>\n",
      "models                module            <module 'models' from 'c:<...>de\\\\code2vec\\\\models.py'>\n",
      "n_training_examples   int               665115\n",
      "nn                    module            <module 'torch.nn' from '<...>\\torch\\\\nn\\\\__init__.py'>\n",
      "optim                 module            <module 'torch.optim' fro<...>rch\\\\optim\\\\__init__.py'>\n",
      "os                    module            <module 'os' (frozen)>\n",
      "p                     str               158013566\n",
      "path2count            dict              n=807137\n",
      "path2idx              dict              n=807139\n",
      "random                module            <module 'random' from 'c:<...>aconda3\\\\Lib\\\\random.py'>\n",
      "summary               function          <function summary at 0x000001E63613A3E0>\n",
      "sys                   module            <module 'sys' (built-in)>\n",
      "t                     Tensor            tensor([[116998,  30192, <...>     1,      1,      1]])\n",
      "target2count          dict              n=199747\n",
      "target2idx            dict              n=199749\n",
      "test_loader           DataLoader        <torch.utils.data.dataloa<...>ct at 0x000001E63642F410>\n",
      "torch                 module            <module 'torch' from 'C:\\<...>ges\\\\torch\\\\__init__.py'>\n",
      "tqdm                  type              <class 'tqdm.notebook.tqdm_notebook'>\n",
      "train_loader          DataLoader        <torch.utils.data.dataloa<...>ct at 0x000001E62EDD78F0>\n",
      "v                     int               8\n",
      "w                     str               reversestringhello\n",
      "word2count            dict              n=507270\n",
      "word2idx              dict              n=507272\n"
     ]
    }
   ],
   "source": [
    "%whos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037fb7d",
   "metadata": {},
   "source": [
    "# Instanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5957a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints\\java-small-01-model.pt\n"
     ]
    }
   ],
   "source": [
    "model = models.Code2Vec(\n",
    "    nodes_dim=      len(word2idx),      # nb de \"var\"\n",
    "    paths_dim=      len(path2idx),      # nb de path\n",
    "    embedding_dim=  EMBEDDING_DIM,      # à découpé\n",
    "    output_dim=     len(target2idx),    # nb de classe\n",
    "    dropout=        DROPOUT).to(device)\n",
    "\n",
    "if LOAD:\n",
    "    logfunc(f'Loading model from {MODEL_SAVE_PATH}')\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5d86f",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "226a7bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Code2Vec(\n",
      "  (node_embedding): Embedding(507272, 128)\n",
      "  (path_embedding): Embedding(807139, 128)\n",
      "  (out): Linear(in_features=128, out_features=199749, bias=False)\n",
      "  (do): Dropout(p=0.25, inplace=False)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logfunc(f\"Model structure: {model}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b3ada2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------------------------------+-----------------+-------------+\n",
      "| Layer                       | Input Shape                        | Output Shape    | #Param      |\n",
      "|-----------------------------+------------------------------------+-----------------+-------------|\n",
      "| Code2Vec                    | [128, 200], [128, 200], [128, 200] | [128, 199749]   | 193,861,760 |\n",
      "| ├─(node_embedding)Embedding | [128, 200]                         | [128, 200, 128] | 64,930,816  |\n",
      "| ├─(path_embedding)Embedding | [128, 200]                         | [128, 200, 128] | 103,313,792 |\n",
      "| ├─(out)Linear               | [128, 128]                         | [128, 199749]   | 25,567,872  |\n",
      "| ├─(do)Dropout               | [128, 200, 384]                    | [128, 200, 384] | --          |\n",
      "+-----------------------------+------------------------------------+-----------------+-------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    a=i\n",
    "    break\n",
    "logfunc(summary(model, *[b.to(device) for b in a][1:]))\n",
    "logfunc(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6db6e6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([128]),\n",
       " torch.Size([128, 200]),\n",
       " torch.Size([128, 200]),\n",
       " torch.Size([128, 200])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d152f5f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37327b2",
   "metadata": {},
   "source": [
    "## métrique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a814d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(fx, y):\n",
    "    \"\"\"\n",
    "    Calculate top-1 accuracy\n",
    "\n",
    "    fx = [batch size, output dim]\n",
    "     y = [batch size]\n",
    "    \"\"\"\n",
    "    pred_idxs = fx.max(1, keepdim=True)[1]\n",
    "    correct = pred_idxs.eq(y.view_as(pred_idxs)).sum()\n",
    "    acc = correct.float()/pred_idxs.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def calculate_f1(fx, y):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall and F1 score\n",
    "    - Takes top-1 predictions\n",
    "    - Converts to strings\n",
    "    - Splits into sub-tokens\n",
    "    - Calculates TP, FP and FN\n",
    "    - Calculates precision, recall and F1 score\n",
    "\n",
    "    fx = [batch size, output dim]\n",
    "     y = [batch size]\n",
    "    \"\"\"\n",
    "    pred_idxs = fx.max(1, keepdim=True)[1]\n",
    "    pred_names = [idx2target[i.item()] for i in pred_idxs]\n",
    "    original_names = [idx2target[i.item()] for i in y]\n",
    "    true_positive, false_positive, false_negative = 0, 0, 0\n",
    "    for p, o in zip(pred_names, original_names):\n",
    "        predicted_subtokens = p.split('|')\n",
    "        original_subtokens = o.split('|')\n",
    "        for subtok in predicted_subtokens:\n",
    "            if subtok in original_subtokens:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "        for subtok in original_subtokens:\n",
    "            if not subtok in predicted_subtokens:\n",
    "                false_negative += 1\n",
    "    try:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        precision, recall, f1 = 0, 0, 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def get_metrics(tensor_n, tensor_l, tensor_p, tensor_r, model, criterion):\n",
    "    \"\"\"\n",
    "    Takes inputs, calculates loss, accuracy and other metrics, then calculates gradients and updates parameters\n",
    "\n",
    "    if optimizer is None, then we are doing evaluation so no gradients are calculated and no parameters are updated\n",
    "    \"\"\"\n",
    "\n",
    "    fx = model(tensor_l, tensor_p, tensor_r)\n",
    "\n",
    "    loss = criterion(fx, tensor_n)\n",
    "\n",
    "    acc = calculate_accuracy(fx, tensor_n)\n",
    "    precision, recall, f1 = calculate_f1(fx, tensor_n)\n",
    "\n",
    "    return loss, acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0cdfd",
   "metadata": {},
   "source": [
    "## Eval func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31af8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluation loop using DataLoader.\n",
    "    Wraps computations in `torch.no_grad()` to avoid unnecessary gradient calculations.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    epoch_loss, epoch_acc, epoch_p, epoch_r, epoch_f1 = 0, 0, 0, 0, 0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tensor_n, tensor_l, tensor_p, tensor_r in tqdm(eval_loader, desc=\"eval for {eval_loader.desc} batch\", position=1):\n",
    "            # Move tensors to GPU\n",
    "            tensor_n = tensor_n.to(device, non_blocking=True)\n",
    "            tensor_l = tensor_l.to(device, non_blocking=True)\n",
    "            tensor_p = tensor_p.to(device, non_blocking=True)\n",
    "            tensor_r = tensor_r.to(device, non_blocking=True)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # Forward pass\n",
    "            loss, acc, p, r, f1 = get_metrics(tensor_n, tensor_l, tensor_p, tensor_r, model, criterion)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "            epoch_p += p\n",
    "            epoch_r += r\n",
    "            epoch_f1 += f1\n",
    "            n_batches += 1\n",
    "\n",
    "            if n_batches % LOG_EVERY == 0:\n",
    "                log = f\"\\t| Batches: {n_batches} |\\n\"\n",
    "                log += f\"\\t| Loss: {epoch_loss / n_batches:.3f} | Acc.: {epoch_acc / n_batches:.3f} | P: {epoch_p / n_batches:.3f} | R: {epoch_r / n_batches:.3f} | F1: {epoch_f1 / n_batches:.3f}\"\n",
    "                logfunc(log)\n",
    "\n",
    "    return epoch_loss / n_batches, epoch_acc / n_batches, epoch_p / n_batches, epoch_r / n_batches, epoch_f1 / n_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd90ca2",
   "metadata": {},
   "source": [
    "## Training func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92ef6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop using DataLoader for batch streaming\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss, epoch_acc, epoch_p, epoch_r, epoch_f1 = 0, 0, 0, 0, 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for tensor_n, tensor_l, tensor_p, tensor_r in tqdm(train_loader, desc=\"batch - trainning\", position=1):\n",
    "        # Move tensors to GPU\n",
    "        tensor_n = tensor_n.to(device, non_blocking=True)\n",
    "        tensor_l = tensor_l.to(device, non_blocking=True)\n",
    "        tensor_p = tensor_p.to(device, non_blocking=True)\n",
    "        tensor_r = tensor_r.to(device, non_blocking=True)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        # loss, acc, p, r, f1 = get_metrics(tensor_n, tensor_l, tensor_p, tensor_r, model, criterion)\n",
    "        \n",
    "        fx = model(tensor_l, tensor_p, tensor_r)\n",
    "        loss = criterion(fx, tensor_n)\n",
    "        acc = calculate_accuracy(fx, tensor_n)\n",
    "        p, r, f1 = calculate_f1(fx, tensor_n)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics\n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        epoch_p += p\n",
    "        epoch_r += r\n",
    "        epoch_f1 += f1\n",
    "        n_batches += 1\n",
    "\n",
    "        if n_batches % LOG_EVERY == 0:\n",
    "            log = f\"\\t| Batches: {n_batches} | Completion: {((n_batches*BATCH_SIZE)/len(data_train))*100:.3f}% |\\n\"\n",
    "            log += f\"\\t| Loss: {epoch_loss / n_batches:.3f} | Acc.: {epoch_acc / n_batches:.3f} | P: {epoch_p / n_batches:.3f} | R: {epoch_r / n_batches:.3f} | F1: {epoch_f1 / n_batches:.3f}\"\n",
    "            logfunc(log)\n",
    "\n",
    "    return epoch_loss / n_batches, epoch_acc / n_batches, epoch_p / n_batches, epoch_r / n_batches, epoch_f1 / n_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d5847",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1da65bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "collected = gc.collect()\n",
    "print(collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc8a25ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88355483fea747cba91d7712601f5ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44850f847efb497db19aa7a23c62e385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch - trainning:   0%|          | 0/5197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t| Batches: 1000 | Completion: 19.245% |\n",
      "\t| Loss: 9.027 | Acc.: 0.168 | P: 0.321 | R: 0.240 | F1: 0.274\n",
      "\t| Batches: 2000 | Completion: 38.490% |\n",
      "\t| Loss: 9.090 | Acc.: 0.174 | P: 0.332 | R: 0.244 | F1: 0.281\n",
      "\t| Batches: 3000 | Completion: 57.734% |\n",
      "\t| Loss: 9.009 | Acc.: 0.183 | P: 0.346 | R: 0.252 | F1: 0.292\n",
      "\t| Batches: 4000 | Completion: 76.979% |\n",
      "\t| Loss: 8.919 | Acc.: 0.190 | P: 0.356 | R: 0.259 | F1: 0.300\n",
      "\t| Batches: 5000 | Completion: 96.224% |\n",
      "\t| Loss: 8.814 | Acc.: 0.197 | P: 0.366 | R: 0.267 | F1: 0.308\n",
      "Epoch: 03 - Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f981f062a32f4b28a55790974476adb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval for {eval_loader.desc} batch:   0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 03 |\n",
      "| Train Loss: 8.793 | Train Precision: 0.367 | Train Recall: 0.268 | Train F1: 0.310 | Train Acc: 19.81% |\n",
      "| Val. Loss: 12.142 | Val. Precision: 0.095 | Val. Recall: 0.126 | Val. F1: 0.108 | Val. Acc: 6.10% |\n",
      "Epoch: 04 - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cf9251037947228397a605221ef0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batch - trainning:   0%|          | 0/5197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(START_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, N_EPOCHS), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m      4\u001b[0m     logfunc(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     train_loss, train_acc, train_p, train_r, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     logfunc(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Validation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     valid_loss, valid_acc, valid_p, valid_r, valid_f1 \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[35], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m tensor_p \u001b[38;5;241m=\u001b[39m tensor_p\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m tensor_r \u001b[38;5;241m=\u001b[39m tensor_r\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\cuda\\__init__.py:985\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    983\u001b[0m _lazy_init()\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(START_EPOCHS+1, N_EPOCHS), desc=\"epoch\", position=0):\n",
    "    logfunc(f\"Epoch: {epoch+1:02} - Training\")\n",
    "    train_loss, train_acc, train_p, train_r, train_f1 = train(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    logfunc(f\"Epoch: {epoch+1:02} - Validation\")\n",
    "    valid_loss, valid_acc, valid_p, valid_r, valid_f1 = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, f'{DATASET_NAME}-{epoch:02}-model.pt'))\n",
    "\n",
    "    log = f\"| Epoch: {epoch+1:02} |\\n\"\n",
    "    log += f\"| Train Loss: {train_loss:.3f} | Train Precision: {train_p:.3f} | Train Recall: {train_r:.3f} | Train F1: {train_f1:.3f} | Train Acc: {train_acc * 100:.2f}% |\\n\"\n",
    "    log += f\"| Val. Loss: {valid_loss:.3f} | Val. Precision: {valid_p:.3f} | Val. Recall: {valid_r:.3f} | Val. F1: {valid_f1:.3f} | Val. Acc: {valid_acc * 100:.2f}% |\"\n",
    "    logfunc(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7452ba",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba75b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "logfunc('Testing')\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "test_loss, test_acc, test_p, test_r, test_f1 = evaluate(model, eval_loader, criterion, device)\n",
    "\n",
    "logfunc(f'| Test Loss: {test_loss:.3f} | Test Precision: {test_p:.3f} | Test Recall: {test_r:.3f} | Test F1: {test_f1:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
